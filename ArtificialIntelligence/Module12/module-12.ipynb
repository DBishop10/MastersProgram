{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 12 - Programming Assignment\n",
    "\n",
    "## General Directions\n",
    "\n",
    "1. You must follow the Programming Requirements outlined on Canvas.\n",
    "2. The Notebook should be cleanly and fully executed before submission.\n",
    "3. You should change the name of this file to be your JHED id. For example, `jsmith299.ipynb` although Canvas will change it to something else...\n",
    "4. You must follow the Programming Requirments for this course.\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        You should always read the entire assignment before beginning your work, so that you know in advance what the requested output will be and can plan your implementation accordingly.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div style=\"color: white; background: #C83F49; margin:20px; padding: 20px;\">\n",
    "    <strong>Academic Integrity and Copyright</strong>\n",
    "    <p>You are not permitted to consult outside sources (Stackoverflow, YouTube, ChatGPT, etc.) or use \"code assistance\" (Co-Pilot, etc) to complete this assignment. By submitting this assignment for grading, you certify that the submission is 100% your own work, based on course materials, group interactions, instructor guidance. You agree to comply by the requirements set forth in the Syllabus, including, by reference, the JHU KSAS/WSE Graduate Academic Misconduct Policy.</p>\n",
    "    <p>Sharing this assignment either directly (e.g., email, github, homework site) or indirectly (e.g., ChatGPT, machine learning platform) is a violation of the copyright. Additionally, all such sharing is a violation the Graduate Academic Misconduct Policy (facilitating academic dishonesty is itself academic dishonesty), even after you graduate.</p>\n",
    "    <p>If you have questions or if you're unsure about the policy, ask via Canvas Inbox. In this case, being forgiven is <strong>not</strong> easier than getting permission and ignorance is not an excuse.</p>\n",
    "    <p>This assignment is copyright (&copy Johns Hopkins University &amp; Stephyn G. W. Butcher). All rights reserved.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k Nearest Neighbors and Model Evaluation\n",
    "\n",
    "In this programming assignment you will use k Nearest Neighbors (kNN) to build a \"model\" that will estimate the compressive strength of various types of concrete. This assignment has several objectives:\n",
    "\n",
    "1. Implement the kNN algorithm with k=9. Remember...the data + distance function is the model in kNN. In addition to asserts that unit test your code, you should \"test drive\" the model, showing output that a non-technical person could interpret.\n",
    "\n",
    "2. You are going to compare the kNN model above against the baseline model described in the course notes (the mean of the training set's target variable). You should use 5 fold cross validation and Mean Squared Error (MSE):\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum^n_i (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "as the evaluation metric (\"error\"). Refer to the course notes for the format your output should take. Don't forget a discussion of the results.\n",
    "\n",
    "3. use validation curves to tune a *hyperparameter* of the model. \n",
    "In this case, the hyperparameter is *k*, the number of neighbors. Don't forget a discussion of the results.\n",
    "\n",
    "4. evaluate the *generalization error* of the new model.\n",
    "Because you may have just created a new, better model, you need a sense of its generalization error, calculate that. Again, what would you like to see as output here? Refer to the course notes. Don't forget a discussion of the results. Did the new model do better than either model in Q2?\n",
    "\n",
    "5. pick one of the \"Choose Your Own Adventure\" options.\n",
    "\n",
    "Refer to the \"course notes\" for this module for most of this assignment.\n",
    "Anytime you just need test/train split, use fold index 0 for the test set and the remainder as the training set.\n",
    "Discuss any results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "The function `parse_data` loads the data from the specified file and returns a List of Lists. The outer List is the data set and each element (List) is a specific observation. Each value of an observation is for a particular measurement. This is what we mean by \"tidy\" data.\n",
    "\n",
    "The function also returns the *shuffled* data because the data might have been collected in a particular order that *might* bias training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Dict, Tuple, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file_name: str) -> List[List]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = [float(value) for value in line.rstrip().split(\",\")]\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parse_data(\"concrete_compressive_strength.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[141.3, 212.0, 0.0, 203.5, 0.0, 971.8, 748.5, 28.0, 29.89]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1,030 observations and each observation has 8 measurements. The data dictionary for this data set tells us the definitions of the individual variables (columns/indices):\n",
    "\n",
    "| Index | Variable | Definition |\n",
    "|-------|----------|------------|\n",
    "| 0     | cement   | kg in a cubic meter mixture |\n",
    "| 1     | slag     | kg in a cubic meter mixture |\n",
    "| 2     | ash      | kg in a cubic meter mixture |\n",
    "| 3     | water    | kg in a cubic meter mixture |\n",
    "| 4     | superplasticizer | kg in a cubic meter mixture |\n",
    "| 5     | coarse aggregate | kg in a cubic meter mixture |\n",
    "| 6     | fine aggregate | kg in a cubic meter mixture |\n",
    "| 7     | age | days |\n",
    "| 8     | concrete compressive strength | MPa |\n",
    "\n",
    "The target (\"y\") variable is a Index 8, concrete compressive strength in (Mega?) [Pascals](https://en.wikipedia.org/wiki/Pascal_(unit))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Splits - n folds\n",
    "\n",
    "With n fold cross validation, we divide our data set into n subgroups called \"folds\" and then use those folds for training and testing. You pick n based on the size of your data set. If you have a small data set--100 observations--and you used n=10, each fold would only have 10 observations. That's probably too small. You want at least 30. At the other extreme, we generally don't use n > 10.\n",
    "\n",
    "With 1,030 observations, n = 10 is fine so we will have 10 folds.\n",
    "`create_folds` will take a list (xs) and split it into `n` equal folds with each fold containing one-tenth of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(xs: List, n: int) -> List[List[List]]:\n",
    "    k, m = divmod(len(xs), n)\n",
    "    # be careful of generators...\n",
    "    return list(xs[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = create_folds(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always use one of the n folds as a test set (and, sometimes, one of the folds as a *pruning* set but not for kNN), and the remaining folds as a training set.\n",
    "We need a function that'll take our n folds and return the train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(folds: List[List[List]], index: int) -> Tuple[List[List], List[List]]:\n",
    "    training = []\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = training + fold\n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the function to give us a train and test datasets where the test set is the fold at index 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = create_train_test(folds, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "927"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers\n",
    "\n",
    "Answer the questions above in the space provided below, adding cells as you need to.\n",
    "Put everything in the helper functions and document them.\n",
    "Document everything (what you're doing and why).\n",
    "If you're not sure what format the output should take, refer to the course notes and what they do for that particular topic/algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: kNN\n",
    "\n",
    "Implement k Nearest Neighbors algorithm with k = 9. (Do not confuse the algorithm with evaluating the algorithm. We just want the algorithm here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `kNN` <a id=\"kNN\"></a>\n",
    "\n",
    "**Description:**  \n",
    "This function implements the k-Nearest Neighbors (kNN) algorithm. Unlike many models, kNN does not require a training phase. Instead, the model processes both training and testing data simultaneously to make predictions. It calculates distances between the test points and the training points, identifies the k nearest neighbors, and predicts based on their values.\n",
    "\n",
    "**Parameters:**  \n",
    "- `training_data` (`List[List[float]]`): The training data for the model to utilize for predicting the test_data.\n",
    "- `test_data` (`List[List[float]]`): The testing data for the model to predict.\n",
    "- `k` (`int`): The number of nearest neighbors to consider for prediction.\n",
    "\n",
    "**Returns:**\n",
    "- `preds` (`List[float]`): The predicted values of test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN(training_data: List[List[float]], test_data: List[List[float]], k: int = 9) -> List[float]: \n",
    "    \n",
    "    if k > len(training_data):\n",
    "        print(f\"k of {k} is larger than training data, this will not result in good data, use a smaller k\")\n",
    "        return []\n",
    "\n",
    "    preds = []\n",
    "    \n",
    "    for test_point in test_data:\n",
    "        distances = [\n",
    "            (train_point, sum((a - b) ** 2 for a, b in zip(test_point[:-1], train_point[:-1])) ** 0.5)\n",
    "            for train_point in training_data\n",
    "        ]\n",
    "        neighbors = sorted(distances, key=lambda x: x[1])[:k]\n",
    "        preds.append(sum(neighbor[0][-1] for neighbor in neighbors) / k)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k of 4 is larger than training data, this will not result in good data, use a smaller k\n"
     ]
    }
   ],
   "source": [
    "training_data = [[1.0, 1.0], [4.0, 4.0], [8.0, 8.0]]\n",
    "\n",
    "test_data = [[1.5], [7.5]]\n",
    "\n",
    "predictions = kNN(training_data, test_data, k=4)\n",
    "assert(predictions == []) # k too large shouldnt predict\n",
    "\n",
    "predictions = kNN(training_data, test_data, k=1)\n",
    "assert(len(predictions) == 2) # Should only predict for 2 datapoints\n",
    "assert(type(predictions[0]) == float) #Should be returning floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Evaluation vs. The Mean\n",
    "\n",
    "Using Mean Squared Error (MSE) as your evaluation metric, evaluate your implement above and the Null model, the mean. See the notes for the format of the output.\n",
    "\n",
    "For this part of the assignment, the Programming Requirements are a bit difficult (although not impossible) to follow in terms of *testing*. If you can figure out how to test your code, that's best but if you can't, for this part of the assignment, that's ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `mean_squared_error` <a id=\"mean_squared_error\"></a>\n",
    "\n",
    "**Description:**  \n",
    "This function calculates the Mean Squared Error (MSE) between the true values (`y_true`) and the predicted values (`y_pred`). This is important for kNN as it is a simple and effective evaluation metric. Smaller values indicate better performance.\n",
    "\n",
    "**Parameters:**  \n",
    "- `y_true` (`List[float]`): True values of y\n",
    "- `y_pred` (`List[float]`): Predicted values of y to compare to true\n",
    "\n",
    "**Returns:**\n",
    "- `mse` (`float`): MSE of the models predicted and true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true: List[float], y_pred: List[float]) -> float:\n",
    "    return sum((true - pred) ** 2 for true, pred in zip(y_true, y_pred)) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [1.0, 2.0, 3.0]\n",
    "y_pred = [1.0, 2.0, 3.0]\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "assert(mse == 0.0) # Exact same so should be 0\n",
    "\n",
    "y_pred = [1.1, 2.1, 3.1]\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "assert round(mse, 2) == 0.01 # Each error squared is 0.01, averaged over 3 samples\n",
    "\n",
    "y_pred = [2.0, 3.0, 4.0]\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "assert(mse == 1.0) # Each value is 1.0 over actual / 3 which should end up with 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):\n",
      "  Null Model: 233.1731\n",
      "  kNN (k=9): 64.6342\n"
     ]
    }
   ],
   "source": [
    "y_train = [row[-1] for row in train]  # Training targets\n",
    "y_test = [row[-1] for row in test]   # Test targets\n",
    "\n",
    "null_predictions = [sum(y_train) / len(y_train)] * len(y_test) # My understanding of null model is using the mean of the data\n",
    "\n",
    "knn_predictions = kNN(train, test, 9)\n",
    "\n",
    "null_mse = mean_squared_error(y_test, null_predictions)\n",
    "knn_mse = mean_squared_error(y_test, knn_predictions)\n",
    "\n",
    "# Output results\n",
    "print(f\"Mean Squared Error (MSE):\")\n",
    "print(f\"  Null Model: {null_mse:.4f}\")\n",
    "print(f\"  kNN (k={9}): {knn_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Hyperparameter Tuning\n",
    "\n",
    "Tune the value of k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `hyperparameter_tuning` <a id=\"hyperparameter_tuning\"></a>\n",
    "\n",
    "**Description:**  \n",
    "This function performs hyperparameter tuning for the k-Nearest Neighbors (kNN) algorithm by evaluating different values of k to find the one that minimizes the Mean Squared Error (MSE) on a given test dataset. This is incredibly important to the kNN function to find the best k value so we are not too badly overfitting or underfiting, making predictions better.\n",
    "\n",
    "**Parameters:**  \n",
    "- `train` (`List[List[float]]`): Training dataset for kNN to utilize to predict testing point\n",
    "- `test` (`List[List[float]]`): Testing dataset to predict values for\n",
    "- `k_values` (`List[int]`): Different values of k to try and determine the best one\n",
    "\n",
    "**Returns:**\n",
    "- `k_best` (`int`): The best found value of k for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(train: List[List[float]], test: List[List[float]], k_values: List[int], debug: bool=False) -> int:\n",
    "\n",
    "    best_k = None\n",
    "    best_mse = float('inf') \n",
    "    for k in k_values:\n",
    "        knn_predictions = kNN(train, test, k)\n",
    "        y_test = [row[-1] for row in test]\n",
    "        mse = mean_squared_error(y_test, knn_predictions)\n",
    "        if(debug):\n",
    "            print(f\"k={k}, MSE={mse:.4f}\")\n",
    "        \n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_k = k\n",
    "    if(debug):\n",
    "        print(f\"Best k: {best_k} with MSE: {best_mse:.4f}\")\n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k of 4 is larger than training data, this will not result in good data, use a smaller k\n",
      "k of 5 is larger than training data, this will not result in good data, use a smaller k\n"
     ]
    }
   ],
   "source": [
    "train_data = [\n",
    "    [1.0, 1.0],\n",
    "    [2.0, 2.0], \n",
    "    [3.0, 3.0]   \n",
    "]\n",
    "\n",
    "test_data = [\n",
    "    [1.5, 1.5],  \n",
    "    [2.5, 2.5]\n",
    "]\n",
    "\n",
    "k_values = [1, 2, 3]\n",
    "\n",
    "best_k = hyperparameter_tuning(train_data, test_data, k_values)\n",
    "assert(best_k == 2) # Should be k = 2\n",
    "assert(type(best_k) == int) # k should be an integer no matter what\n",
    "k_values = [1, 2, 3, 4, 5]\n",
    "best_k = hyperparameter_tuning(train_data, test_data, k_values)\n",
    "assert(best_k is not None) # best k should return even if k values go over training data size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1, MSE=69.0425\n",
      "k=2, MSE=72.5728\n",
      "k=3, MSE=57.5925\n",
      "k=4, MSE=63.8717\n",
      "k=5, MSE=67.6430\n",
      "k=6, MSE=71.5821\n",
      "k=7, MSE=69.6825\n",
      "k=8, MSE=66.9971\n",
      "k=9, MSE=64.6342\n",
      "k=10, MSE=66.3015\n",
      "k=11, MSE=68.7963\n",
      "k=12, MSE=72.0420\n",
      "k=13, MSE=72.5542\n",
      "k=14, MSE=74.2889\n",
      "k=15, MSE=75.9650\n",
      "k=16, MSE=78.4388\n",
      "k=17, MSE=79.7309\n",
      "k=18, MSE=79.5882\n",
      "k=19, MSE=80.2391\n",
      "k=20, MSE=81.5264\n",
      "Best k: 3 with MSE: 57.5925\n",
      "The best k value is 3\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "k_values = list(range(1, 21))  \n",
    "best_k = hyperparameter_tuning(train, test, k_values, True)\n",
    "print(f\"The best k value is {best_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Generalization Error\n",
    "\n",
    "Analyze and discuss the generalization error of your model with the value of k from Problem 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `generalization_error` <a id=\"generalization_error\"></a>\n",
    "\n",
    "**Description:**  \n",
    "This function evaluates the generalization error of a k-Nearest Neighbors (kNN) model by comparing its performance on the training and test datasets. It calculates the Mean Squared Error (MSE) for both sets and computes the generalization gap, which measures the difference in performance between the training and test datasets. This is an important function for kNN to determine how good it is at determining new data compared to the data it already has/\"trained\" on.\n",
    "\n",
    "**Parameters:**  \n",
    "- `train` (`List[List[float]]`): Training data for the dataset\n",
    "- `test` (`List[List[float]]`): Testing data for the dataset\n",
    "- `best_k` (`int`): Best found value of k for the datase\n",
    "\n",
    "**Returns:**\n",
    "- `train_mse, test_mse` (`tuple[float, float]`): The training MSE and testing MSE that we calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalization_error(train: List[List[float]], test: List[List[float]], best_k: int) -> tuple[float, float]:\n",
    "    y_train = [row[-1] for row in train]\n",
    "    y_test = [row[-1] for row in test]\n",
    "\n",
    "    train_predictions = kNN(train, train, best_k)\n",
    "    test_predictions = kNN(train, test, best_k)\n",
    "\n",
    "    train_mse = mean_squared_error(y_train, train_predictions)\n",
    "    test_mse = mean_squared_error(y_test, test_predictions)\n",
    "\n",
    "    print(f\"Generalization Error Analysis (k={best_k}):\")\n",
    "    print(f\"  Training Error (MSE): {train_mse:.4f}\")\n",
    "    print(f\"  Test Error (MSE): {test_mse:.4f}\")\n",
    "    print(f\"  Generalization Gap: {abs(test_mse - train_mse):.4f}\")\n",
    "    return train_mse, test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalization Error Analysis (k=1):\n",
      "  Training Error (MSE): 0.0000\n",
      "  Test Error (MSE): 0.0000\n",
      "  Generalization Gap: 0.0000\n",
      "Generalization Error Analysis (k=3):\n",
      "  Training Error (MSE): 0.6667\n",
      "  Test Error (MSE): 2.0000\n",
      "  Generalization Gap: 1.3333\n"
     ]
    }
   ],
   "source": [
    "train_data = [[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]]\n",
    "test_data = [[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]]\n",
    "best_k = 1\n",
    "\n",
    "train_mse, test_mse = generalization_error(train_data, test_data, best_k)\n",
    "assert(train_mse == 0.0) # Should be perfect as its linear\n",
    "assert(test_mse == train_mse) # Should be exact same as train since same dataset\n",
    "test_data = [[1.5, 1.0], [2.5, 4.0], [3.5, 3.0]]\n",
    "best_k = 3\n",
    "train_mse, test_mse = generalization_error(train_data, test_data, best_k)\n",
    "assert(test_mse > train_mse) # train_mse should be near perfect as its linear, test data should be bad as its not linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalization Error Analysis (k=3):\n",
      "  Training Error (MSE): 35.8110\n",
      "  Test Error (MSE): 57.5925\n",
      "  Generalization Gap: 21.7815\n"
     ]
    }
   ],
   "source": [
    "generalization_mse = generalization_error(train, test, best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "As we can see we have a generalization gap of 21.7815 between the training and testing data. The trainig error is reasonably low while the testing error is higher suggesting that the model is struggling more on generalizing to unseen data compared to seen data. This would be an indication of overfitting. Other k values will have different results. In previous iterations I got k=1 which had a generalization gap of over 80 as its training error was near 0 and its testing error was quite high. Each k will have differing results on the generalization ability of this model, more data may help or may make it worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Choose your own adventure\n",
    "\n",
    "You have two options for the next part:\n",
    "\n",
    "1. You can implement mean normalization (also called \"z-score standardization\") of the *features*; **do not** normalize the target, y. See if this improves the generalization error of your model (middle).\n",
    "\n",
    "2. You can implement *learning curves* to see if more data would likely improve your model (easiest).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `standardize_features` <a id=\"standardize_features\"></a>\n",
    "\n",
    "**Description:**  \n",
    "This function standardizes the features of a dataset by subtracting the mean and dividing by the standard deviation for each feature. This is done in hopes to imporve the performance of kNN as it ensures all features are on the same scale.\n",
    "\n",
    "**Parameters:**  \n",
    "- `data` (`List[List[float]]`): Data to standardize\n",
    "- `means` (`List[float]`): Mean value for each feature\n",
    "- `stds` (`List[float]`): Standard Deviation values for each feature\n",
    "\n",
    "**Returns:**\n",
    "- `standardized_data` (`List[List[float]]`): Dataset that has been standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features(data: List[List[float]], means: List[float], stds: List[float]) -> List[List[float]]:\n",
    "    standardized_data = []\n",
    "    for row in data:\n",
    "        standardized_row = [\n",
    "            (value - mean) / std if std > 0 else 0\n",
    "            for value, mean, std in zip(row[:-1], means, stds)\n",
    "        ]\n",
    "        standardized_data.append(standardized_row + [row[-1]]) \n",
    "    return standardized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [2.0, 4.0, 1.0], \n",
    "    [3.0, 6.0, 0.0] \n",
    "]\n",
    "\n",
    "means = [2.5, 5.0]\n",
    "stds = [0.5, 1.0]\n",
    "\n",
    "expected = [\n",
    "    [-1.0, -1.0, 1.0], \n",
    "    [1.0, 1.0, 0.0]    \n",
    "]\n",
    "\n",
    "standardized = standardize_features(data, means, stds)\n",
    "assert(type(standardized) == list) # Should return a list\n",
    "assert(standardized == expected) # Should equal expected based on my calcs\n",
    "means = [2.0, 5.0]\n",
    "standardized2 = standardize_features(data, means, stds)\n",
    "assert(not standardized == standardized2) # As means have changed the standardization should as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `mean_and_std` <a id=\"mean_and_std\"></a>\n",
    "\n",
    "**Description:**  \n",
    "This function computes the mean and standard deviation for each feature in a dataset. The target column (assumed to be the last column) is excluded from the calculations. Needed to perform standardization.\n",
    "\n",
    "**Parameters:**  \n",
    "- `data` (`List[List[float]]`): dataset to get the mean and standard deviations of\n",
    "\n",
    "**Returns:**\n",
    "- `means, stds` (`Tuple[List[float], List[float]]`): The list of means and standardizations for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def mean_and_std(data: List[List[float]]) -> Tuple[List[float], List[float]]:\n",
    "    num_features = len(data[0]) - 1  # Exclude target column\n",
    "    means = [sum(row[i] for row in data) / len(data) for i in range(num_features)]\n",
    "    stds = [\n",
    "        math.sqrt(sum((row[i] - means[i]) ** 2 for row in data) / len(data))\n",
    "        for i in range(num_features)\n",
    "    ]\n",
    "    return means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [2.0, 4.0, 1.0],\n",
    "    [2.0, 4.0, 0.0]\n",
    "]\n",
    "\n",
    "expected_means = [2.0, 4.0]\n",
    "expected_stds = [0.0, 0.0]  \n",
    "\n",
    "means, stds = mean_and_std(data)\n",
    "assert(type(means) == list and type(stds) == list) # They should both be a list\n",
    "assert means == expected_means # Those are the means of the dataset minus y\n",
    "assert stds == expected_stds # Should be no variation in dataset for stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalization Error Analysis (k=4):\n",
      "  Training Error (MSE): 41.7987\n",
      "  Test Error (MSE): 60.1788\n",
      "  Generalization Gap: 18.3800\n"
     ]
    }
   ],
   "source": [
    "means, stds = mean_and_std(train)  # Compute means and stds from training data\n",
    "standardized_train = standardize_features(train, means, stds)  # Standardize training data\n",
    "standardized_test = standardize_features(test, means, stds)  # Standardize test data\n",
    "\n",
    "best_k_standardized = hyperparameter_tuning(standardized_train, standardized_test, k_values=list(range(1, 21)))\n",
    "\n",
    "# Re-run kNN with standardized features\n",
    "generalization_mse = generalization_error(standardized_train, standardized_test, best_k_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "This did not help my training and testing error, it infact made it worse for both. However, the generalization gap did go down implying that they are becoming closer together with better generalization for unseen data, just a worse performance overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "en605645",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "117px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
