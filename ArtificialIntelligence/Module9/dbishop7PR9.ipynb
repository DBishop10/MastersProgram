{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "1. Change the name of this file to be your JHED id as in `jsmith299.ipynb`. Because sure you use your JHED ID (it's made out of your name and not your student id which is just letters and numbers).\n",
    "2. Make sure the notebook you submit is cleanly and fully executed. I do not grade unexecuted notebooks.\n",
    "3. Submit your notebook back in Blackboard where you downloaded this file.\n",
    "\n",
    "*Provide the output **exactly** as requested*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "For this assignment you will be implementing and evaluating a Naive Bayes Classifier with the same data from last week:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "(You should have downloaded it).\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        No Pandas. The only acceptable libraries in this class are those contained in the `environment.yml`. No OOP, either. You can use Dicts, NamedTuples, Data Classes, etc. as your abstract data type (ADT).\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "You'll first need to calculate all of the necessary probabilities using a `train` function. A flag will control whether or not you use \"+1 Smoothing\" or not. You'll then need to have a `classify` function that takes your probabilities, a List of instances (possibly a list of 1) and returns a List of Tuples. Each Tuple has the best class in the first position and a dict with a key for every possible class label and the associated *normalized* probability. For example, if we have given the `classify` function a list of 2 observations, we would get the following back:\n",
    "\n",
    "```\n",
    "[(\"e\", {\"e\": 0.98, \"p\": 0.02}), (\"p\", {\"e\": 0.34, \"p\": 0.66})]\n",
    "```\n",
    "\n",
    "when calculating the error rate of your classifier, you should pick the class label with the highest probability; you can write a simple function that takes the Dict and returns that class label.\n",
    "\n",
    "As a reminder, the Naive Bayes Classifier generates the *unnormalized* probabilities from the numerator of Bayes Rule:\n",
    "\n",
    "$$P(C|A) \\propto P(A|C)P(C)$$\n",
    "\n",
    "where C is the class and A are the attributes (data). Since the normalizer of Bayes Rule is the *sum* of all possible numerators and you have to calculate them all, the normalizer is just the sum of the probabilities.\n",
    "\n",
    "You will have the same basic functions as the last module's assignment and some of them can be reused or at least repurposed.\n",
    "\n",
    "`train` takes training_data and returns a Naive Bayes Classifier (NBC) as a data structure. There are many options including namedtuples and just plain old nested dictionaries. **No OOP**.\n",
    "\n",
    "```\n",
    "def train(training_data, smoothing=True):\n",
    "   # returns the \"classifier\" (however you decided to represent the probability tables).\n",
    "```\n",
    "\n",
    "The `smoothing` value defaults to True. You should handle both cases.\n",
    "\n",
    "`classify` takes a NBC produced from the function above and applies it to labeled data (like the test set) or unlabeled data (like some new data). (This is not the same `classify` as the pseudocode which classifies only one instance at a time; it can call it though).\n",
    "\n",
    "```\n",
    "def classify(nbc, observations, labeled=True):\n",
    "    # returns a list of tuples, the argmax and the raw data as per the pseudocode.\n",
    "```\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the classification result and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "Do not use anything else as evaluation metric or the submission will be deemed incomplete, ie, an \"F\". (Hint: accuracy rate is not the error rate!).\n",
    "\n",
    "`cross_validate` takes the data and uses 5x2 cross validation (from Module 2!) to `train`, `classify`, and `evaluate`. **Remember to shuffle your data before you create your folds**. I leave the exact signature of `cross_validate` to you but you should write it so that you can use it with *any* `classify` function of the same form (using higher order functions and partial application). If you did so last time, you can reuse it for this assignment.\n",
    "\n",
    "Following Module 2's materials, `cross_validate` should print out the fold number and the evaluation metric (error rate) for each fold and then the average value (and the variance). What you are looking for here is a consistent evaluation metric cross the folds. You should print the error rates in terms of percents (ie, multiply the error rate by 100 and add \"%\" to the end).\n",
    "\n",
    "To summarize...\n",
    "\n",
    "Apply the Naive Bayes Classifier algorithm to the Mushroom data set using 5x2 cross validation and the error rate as the evaluation metric. You will do this *twice*. Once with smoothing=True and once with smoothing=False. You should follow up with a brief hypothesis/explanation for the similarities or differences in the results. You may also compare the results to the Decision Tree and why you think they're different (if they are)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provided Functions\n",
    "\n",
    "You do not need to document these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this function to read the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random #Had to add this, code was crashing later\n",
    "\n",
    "def parse_data(file_name: str) -> list[list]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = line.rstrip().split(\",\")\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this function to create 10 folds for 5x2 cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(xs: list, n: int) -> list[list[list]]:\n",
    "    k, m = divmod(len(xs), n)\n",
    "    # be careful of generators...\n",
    "    return list(xs[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your code after this line:\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train` <a id=\"train\"></a>\n",
    "\n",
    "**Description:**  \n",
    "This function builds and trains a Naive Baise Classifier with optional Laplace smoothing. It is required for our code as it is what creates the method for Naive Baise to classify. Without this we would not have a classifier of any sort.\n",
    "\n",
    "**Parameters**:\n",
    "- `training_data`: List of lists where each inner list represents an observation and the first element is the class label.\n",
    "- `smoothing`: Boolean indicating whether to use Laplace smoothing.\n",
    "    \n",
    "**Returns**:\n",
    "- A dictionary representing the Naive Bayes Classifier with calculated probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data: List[List[str]], smoothing: bool = True) -> Dict:\n",
    "    class_counts = Counter([data[0] for data in training_data])\n",
    "    total_count = len(training_data)\n",
    "    class_probs = {cls: count / total_count for cls, count in class_counts.items()}\n",
    "    \n",
    "    cond_probs: Dict[str, Dict[int, Dict[str, float]]] = {}\n",
    "\n",
    "    for cls in class_counts:\n",
    "        subset = [data[1:] for data in training_data if data[0] == cls]\n",
    "        cond_probs[cls] = {}  \n",
    "        for i in range(len(subset[0])): \n",
    "            attr_values = [data[i] for data in subset]\n",
    "            attr_counts = Counter(attr_values)\n",
    "            total_attr_count = len(attr_values)\n",
    "\n",
    "            cond_probs[cls][i] = {} \n",
    "            for attr_val, count in attr_counts.items():\n",
    "                if smoothing:\n",
    "                    cond_probs[cls][i][attr_val] = (count + 1) / (total_attr_count + len(attr_counts))\n",
    "                else:\n",
    "                    cond_probs[cls][i][attr_val] = count / total_attr_count\n",
    "\n",
    "    nbc = {\"class_probs\": class_probs,\"cond_probs\": cond_probs}\n",
    "    return nbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [['A', 'x'], ['B', 'y'], ['A', 'x'], ['B', 'y']]\n",
    "nbc = train(data1)\n",
    "assert(isinstance(nbc, dict))  # NBC should be a dictionary\n",
    "\n",
    "data2 = [['A', 'x'], ['A', 'y'], ['A', 'z']]\n",
    "nbc2 = train(data2)\n",
    "assert(nbc2['class_probs'] == {'A': 1.0})  # All labels are A, so the dict should just have A as 1\n",
    "\n",
    "data3 = [['A', 'x', 'y'], ['B', 'x', 'n'], ['A', 'z', 'y'], ['B', 'z', 'n']]\n",
    "nbc3 = train(data3)\n",
    "assert(nbc3['class_probs'] == {'A': .5, 'B':.5}) # Should be an even split for classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `classify` <a id=\"classify\"></a>\n",
    "\n",
    "**Description:**  \n",
    "This function takes a Naive Baise Classifier and a list of observations and returns a list of predicted class labels for each observation. The NBC dictionary is traversed to compute the probability of each class given the observed attribute values, ultimately selecting the class with the highest probability. This is necessary for a NBC to work as it is the function that actually lets us classify datapoints.\n",
    "\n",
    "**Parameters**:  \n",
    "- `nbc` (`dict`): The Naive Bayes Classifier, represented as a dictionary with class_probs for class probabilities and cond_probs for conditional probabilities of attribute values given the class.\n",
    "- `observations` (`List[List[str]]`): A list of observations to classify, where each observation is a list of attribute values.\n",
    "- `labeled` (`bool`): Indicates whether the first element of each observation is the true class label. If `True`, it skips the first element during classification. Defaults to `True`.\n",
    "\n",
    "**Returns**:  \n",
    "- A list of predicted class labels for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(nbc: Dict, observations: List[List[str]], labeled: bool = True):\n",
    "    predictions = []\n",
    "\n",
    "    for observation in observations:\n",
    "        if labeled:\n",
    "            observation = observation[1:]  \n",
    "\n",
    "        class_scores = {}\n",
    "        for cls, cls_prob in nbc[\"class_probs\"].items():\n",
    "            prob = cls_prob \n",
    "            for i, attr_val in enumerate(observation):\n",
    "                if attr_val in nbc[\"cond_probs\"][cls][i]:\n",
    "                    prob *= nbc[\"cond_probs\"][cls][i][attr_val]\n",
    "                else:\n",
    "                    prob *= 0 \n",
    "            class_scores[cls] = prob\n",
    "\n",
    "        total_score = sum(class_scores.values())\n",
    "        normalized_probs = {cls: score / total_score if total_score > 0 else 0.0\n",
    "                            for cls, score in class_scores.items()}\n",
    "        \n",
    "        best_class = max(normalized_probs, key=normalized_probs.get)\n",
    "        \n",
    "        predictions.append((best_class, normalized_probs))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc = {'class_probs': {'A': 0.5, 'B': 0.5}, 'cond_probs': {'A': {0: {'x': 0.5, 'p': 0.5}, 1: {'y': 1.0}}, 'B': {0: {'y': 0.5, 'z': 0.5}, 1: {'n': 1.0}}}}\n",
    "\n",
    "observations1 = [['A', 'x'], ['B', 'y']]\n",
    "assert(classify(nbc, observations1) == [('A', {'A': 1.0, 'B': 0.0}), ('B', {'A': 0.0, 'B': 1.0})])  # Should be a simple A and B 100% likely of each\n",
    "\n",
    "observations2 = [['x'], ['y']]\n",
    "assert(classify(nbc, observations2, labeled=False) == [('A', {'A': 1.0, 'B': 0.0}), ('B', {'A': 0.0, 'B': 1.0})])  # Should classify correctly without labels\n",
    "\n",
    "observations3 = [['A', 'z']]  \n",
    "assert(classify(nbc, observations3) == [('B', {'A': 0.0, 'B': 1.0})])  # Since z not a possible for A, returns it as B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `evaluate` <a id=\"evaluate\"></a>\n",
    "\n",
    "**Description:**  \n",
    "This function calculates the error rate by comparing actual class labels from the dataset with the predicted labels. It counts the number of incorrect predictions and returns the error rate, which is the proportion of wrong predictions. Necessary to test how well our Naive Baise Classifier is performing\n",
    "\n",
    "**Parameters**:  \n",
    "- `data_set` (`List[List[str]]`): The dataset where each inner list represents a data point, and the first element is the actual class label.\n",
    "- `predictions` (`List[tuple]`): A touple of predicted class labels corresponding to the observations in the dataset.\n",
    "\n",
    "**Returns**:  \n",
    "- A float representing the error rate, calculated as the proportion of incorrect predictions out of the total number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_set: List[List[str]], predictions: List[tuple]) -> float:\n",
    "    errors = 0\n",
    "    total = len(data_set)\n",
    "    \n",
    "    for actual, (predicted_class, _) in zip(data_set, predictions):\n",
    "        if actual[0] != predicted_class:\n",
    "            errors += 1\n",
    "\n",
    "    error_rate = errors / total if total > 0 else 0.0\n",
    "    return error_rate * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [['A', 'x'], ['B', 'y'], ['C', 'z']]\n",
    "predictions1 = [('A', {'A': 1.0}), ('B', {'B': 1.0}), ('C', {'C': 1.0})]\n",
    "assert(evaluate(data1, predictions1) == 0.0)  # No errors, error rate should be 0%\n",
    "\n",
    "data2 = [['A', 'x'], ['B', 'y'], ['C', 'z']]\n",
    "predictions2 = [('B', {'B': 0.0}), ('C', {'C': 0.0}), ('B', {'B': 0.0})]\n",
    "assert(evaluate(data2, predictions2) == 100.0)  # All wrong, error rate should be 100%\n",
    "\n",
    "data3 = [['A', 'x'], ['B', 'y'], ['C', 'z'], ['D', 'w']]\n",
    "predictions3 = [('A', {'A': 1.0}), ('B', {'B': 1.0}), ('C', {'C': 1.0}), ('X', {'X': 0.0})]\n",
    "assert(evaluate(data3, predictions3) == 25.0)  # One wrong, error rate should be 25%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `cross_validate` <a id=\"cross_validate\"></a>\n",
    "\n",
    "**Description:**  \n",
    "This function performs k-fold cross-validation on a Naive Baise Classifier. The data is split into `k` folds, and for each iteration, one fold is used as the test set, while the other folds are used for training. The function is important as it trains a Naive Baise Classifier on the training data and then evaluates its accuracy on the test fold multiple times to get a better reading of how the Naive Baise Classifier is performing.\n",
    "\n",
    "**Parameters**:  \n",
    "- `data` (`list[list[str]]`): The dataset, where each inner list represents a data point and the first element of each data point is the class label.\n",
    "- `k` (`int`): The number of folds to split the data into. Defaults to 5.\n",
    "\n",
    "**Returns**:  \n",
    "- A float representing the average error rate across all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(data: List[List[str]], k: int = 5, smoothing=True) -> float:\n",
    "    error_rates = []\n",
    "\n",
    "    for _ in range(2):  \n",
    "        random.shuffle(data) \n",
    "        folds = create_folds(data, k)  \n",
    "\n",
    "        for i in range(k):\n",
    "            test_fold = folds[i]\n",
    "            train_folds = [fold for j, fold in enumerate(folds) if j != i]\n",
    "            train_data = [item for sublist in train_folds for item in sublist]  \n",
    "\n",
    "            nbc = train(train_data, smoothing)\n",
    "\n",
    "            predictions = classify(nbc, test_fold, labeled=True)\n",
    "            error_rate = evaluate(test_fold, predictions)\n",
    "\n",
    "            error_rates.append(error_rate)\n",
    "            print(f\"Fold {i+1} repetition error rate: {error_rate:.2f}%\")\n",
    "\n",
    "    average_error_rate = sum(error_rates) / len(error_rates)\n",
    "    return average_error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 repetition error rate: 100.00%\n",
      "Fold 2 repetition error rate: 100.00%\n",
      "Fold 1 repetition error rate: 100.00%\n",
      "Fold 2 repetition error rate: 100.00%\n",
      "Fold 1 repetition error rate: 0.00%\n",
      "Fold 2 repetition error rate: 0.00%\n",
      "Fold 1 repetition error rate: 100.00%\n",
      "Fold 2 repetition error rate: 100.00%\n",
      "Fold 1 repetition error rate: 0.00%\n",
      "Fold 2 repetition error rate: 0.00%\n",
      "Fold 1 repetition error rate: 0.00%\n",
      "Fold 2 repetition error rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "#Unlike Decision Trees this is a little more random so cant just say X == 0 percent error like for the last assignment\n",
    "data1 = [['A', 'x'], ['B', 'y'], ['A', 'x'], ['B', 'y']]\n",
    "result = cross_validate(data1, k=2)\n",
    "assert(isinstance(result, float)) # Should be a float\n",
    "\n",
    "assert(0 <= cross_validate(data1, k=2) <= 100) # Error Rate shoulnd ever get below 0 or above 100 \n",
    "\n",
    "data3 = [['A', 'x'], ['A', 'x'], ['A', 'x'], ['A', 'x']]\n",
    "assert(cross_validate(data3, k=2) == 0) # All labels being the exact same is the only way to ensure 0% error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation with Laplace smoothing enabled (smoothing=True):\n",
      "Fold 1 repetition error rate: 0.43%\n",
      "Fold 2 repetition error rate: 0.25%\n",
      "Fold 3 repetition error rate: 0.68%\n",
      "Fold 4 repetition error rate: 0.37%\n",
      "Fold 5 repetition error rate: 0.06%\n",
      "Fold 1 repetition error rate: 0.37%\n",
      "Fold 2 repetition error rate: 0.37%\n",
      "Fold 3 repetition error rate: 0.12%\n",
      "Fold 4 repetition error rate: 0.68%\n",
      "Fold 5 repetition error rate: 0.12%\n",
      "Average error rate with smoothing=True: 0.34%\n",
      "\n",
      "Cross-validation with Laplace smoothing disabled (smoothing=False):\n",
      "Fold 1 repetition error rate: 0.62%\n",
      "Fold 2 repetition error rate: 0.06%\n",
      "Fold 3 repetition error rate: 0.18%\n",
      "Fold 4 repetition error rate: 0.49%\n",
      "Fold 5 repetition error rate: 0.25%\n",
      "Fold 1 repetition error rate: 0.43%\n",
      "Fold 2 repetition error rate: 0.12%\n",
      "Fold 3 repetition error rate: 0.43%\n",
      "Fold 4 repetition error rate: 0.25%\n",
      "Fold 5 repetition error rate: 0.55%\n",
      "Average error rate with smoothing=False: 0.34%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = parse_data('data/agaricus-lepiota.data')\n",
    "\n",
    "# Perform 5x2 cross-validation with smoothing=True\n",
    "print(\"Cross-validation with Laplace smoothing enabled (smoothing=True):\")\n",
    "average_error_rate_smoothing = cross_validate(data, k=5)\n",
    "print(f\"Average error rate with smoothing=True: {average_error_rate_smoothing:.2f}%\\n\")\n",
    "\n",
    "# Perform 5x2 cross-validation with smoothing=False\n",
    "print(\"Cross-validation with Laplace smoothing disabled (smoothing=False):\")\n",
    "average_error_rate_no_smoothing = cross_validate(data, k=5, smoothing=False)\n",
    "print(f\"Average error rate with smoothing=False: {average_error_rate_no_smoothing:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Smoothing True vs False\n",
    "If only looking at the averages it would not appear that smoothing did much besides bring the average error rate down slightly (running it a few times the averages were always around 2-4% lower with smoothing=true, sometimes the average would be higher). However after running it a few times I noticed something interesting. Both smoothing false and true would every once in a while get a super high error rate, close to 80%, but only smoothing=true could consistently get an error rate down below 10%. While smoothing=False would only every once in a while give you a percentage data point that was under 10%. Besides that however both seem to get fairly similar results leading me to believe that for this dataset it is not completely necessary to utilize smoothing=true.\n",
    "\n",
    "### Decision Tree vs Naive Baise\n",
    "Overall I can whole heartedly say that the decision tree performed much better on this dataset than Naive Baise performed. This however makes sense, this dataset seems fairly consistent letting the tree make a pretty simple path to determine what is needed for this dataset. Naive Baise does not quite work this way leading to much more error coming out of the model. Most models have pros and cons on how they perform with different datasets and this dataset simply was made more for something like a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "en605645",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
